# Recurrent Neural Network (RNN)

Question: What architectural variants were introduced to address the vanishing gradient problem in standard RNNs?

- [ ] Convolutional Neural Networks (CNN) and Transformer models
- [ ] Long Short-Term Memory (LSTM) and Gated Recurrent Unit (GRU)
- [ ] Feedforward networks and Autoencoder networks
- [ ] Dropout layers and Batch Normalization

<details>
  <summary>Answer</summary>
  <p>Long Short-Term Memory (LSTM) and Gated Recurrent Unit (GRU)</p>
  <p>LSTM and GRU are specialized RNN variants designed specifically to address the vanishing gradient problem that makes it difficult for standard RNNs to learn long-range dependencies. Both architectures use gating mechanisms to control the flow of information through the network, allowing them to selectively remember or forget information over long sequences. This makes them far more effective than standard RNNs for tasks requiring understanding of context across many time steps.</p>
</details>
