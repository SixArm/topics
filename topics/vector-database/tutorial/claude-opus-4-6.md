# Vector database

A vector database is a specialized database system designed to store, index, and query high-dimensional vector embeddings. In the context of modern machine learning and artificial intelligence, data objects such as text, images, audio, and video are transformed into numerical vector representations through embedding models. A vector database provides the infrastructure to persist these embeddings and perform efficient similarity searches across millions or billions of vectors. As AI-driven applications have become central to enterprise technology stacks, vector databases have emerged as a critical component for powering semantic search, recommendation engines, retrieval-augmented generation (RAG) pipelines, and anomaly detection systems.

## How vector databases differ from traditional databases

Traditional relational databases store data in rows and columns, optimized for exact-match lookups and structured queries using SQL. Vector databases operate on a fundamentally different principle. Each record is represented as a point in a high-dimensional space, where proximity between points encodes semantic similarity. Rather than searching for rows that match a precise condition, a vector database finds the nearest neighbors to a given query vector based on a distance or similarity metric.

This architectural distinction means that vector databases require specialized indexing structures and query execution strategies that have no direct analogue in relational or document databases. While a relational database excels at filtering customers by region or retrieving orders by ID, a vector database excels at answering questions like "which product descriptions are most semantically similar to this search query?"

## Vector embeddings

A vector embedding is a fixed-length array of floating-point numbers that encodes the semantic meaning of a data object. Embedding models, typically deep neural networks, are trained to map raw data into a vector space where similar objects are placed close together and dissimilar objects are placed far apart.

- **Text embeddings** are produced by language models such as OpenAI's embedding API, Sentence-BERT, or Cohere Embed. They capture the meaning of words, sentences, or documents.
- **Image embeddings** are generated by vision models such as CLIP or ResNet. They encode visual features like color, texture, shape, and composition.
- **Audio embeddings** come from models such as AudioSet or VGGish, representing spectral and temporal characteristics of sound.
- **Multimodal embeddings** map different data types into a shared vector space, enabling cross-modal search such as finding images that match a text description.

The dimensionality of these embeddings varies by model, commonly ranging from 128 to 4096 dimensions. Higher dimensionality can capture more nuance but increases storage and computational cost.

## Similarity metrics

The choice of similarity metric determines how the database measures the closeness between two vectors. Different metrics suit different embedding types and application requirements.

| Metric | Description | Best suited for |
|---|---|---|
| Cosine similarity | Measures the cosine of the angle between two vectors, ignoring magnitude | Text embeddings, normalized vectors |
| Euclidean distance (L2) | Measures the straight-line distance between two points in vector space | Image embeddings, spatial data |
| Dot product (inner product) | Computes the sum of element-wise products of two vectors | Recommendation systems, embeddings where magnitude carries meaning |
| Manhattan distance (L1) | Sums the absolute differences across all dimensions | Sparse vectors, high-dimensional data with many zero values |
| Hamming distance | Counts the number of positions at which corresponding elements differ | Binary hash codes, compact representations |

Cosine similarity is the most commonly used metric for text-based applications because embedding models often produce vectors where direction encodes meaning and magnitude is less informative.

## Indexing algorithms

Efficient nearest-neighbor search in high-dimensional space is computationally expensive. Exact search requires comparing the query vector against every stored vector, which scales linearly and becomes impractical at large scale. Vector databases address this through approximate nearest neighbor (ANN) algorithms that trade a small amount of accuracy for dramatically faster query performance.

- **HNSW (Hierarchical Navigable Small World)** builds a multi-layer graph structure where each node is connected to its nearest neighbors. Queries traverse the graph from coarse to fine layers, rapidly narrowing the search space. HNSW offers excellent recall and low latency, making it the most widely adopted index type.
- **IVF (Inverted File Index)** partitions the vector space into clusters using k-means or a similar algorithm. At query time, only a subset of clusters is searched, reducing the number of distance calculations. IVF is effective for very large datasets where memory is constrained.
- **LSH (Locality-Sensitive Hashing)** hashes similar vectors into the same bucket with high probability. It is fast and memory-efficient but typically provides lower recall than graph-based methods.
- **PQ (Product Quantization)** compresses vectors by decomposing them into sub-vectors and quantizing each sub-vector independently. This enables approximate distance computation on compressed representations, significantly reducing memory usage.
- **ScaNN (Scalable Nearest Neighbors)** combines quantization with anisotropic scoring to achieve high recall with low latency, particularly effective for datasets in the tens of millions of vectors.

Most production vector databases support multiple index types and allow tuning parameters such as the number of connections per node (for HNSW) or the number of clusters to probe (for IVF) to balance recall, latency, and resource consumption.

## Leading vector database systems

The vector database landscape includes purpose-built systems, extensions to existing databases, and managed cloud services.

| System | Type | Key characteristics |
|---|---|---|
| Pinecone | Managed cloud service | Fully managed, serverless option, integrated metadata filtering, strong developer experience |
| Milvus | Open source | Highly scalable, supports multiple index types (HNSW, IVF, PQ), GPU acceleration, active community |
| Weaviate | Open source | Schema-aware, built-in vectorization modules, GraphQL API, hybrid search combining vectors and keywords |
| Qdrant | Open source | Written in Rust for performance, rich filtering, payload indexing, gRPC and REST APIs |
| Chroma | Open source | Lightweight, designed for AI application prototyping, simple Python API, embeds in applications |
| FAISS | Library (Meta) | Low-level C++ library with Python bindings, extensive index support, not a standalone database |
| pgvector | PostgreSQL extension | Adds vector similarity search to PostgreSQL, leverages existing relational infrastructure |
| Elasticsearch | Search engine with vector support | Dense vector fields with kNN search, combines full-text and vector retrieval in a single system |

The choice among these depends on factors such as scale requirements, operational complexity tolerance, need for managed infrastructure, hybrid search capabilities, and integration with existing technology stacks.

## Core use cases

Vector databases power a broad range of applications across industries.

- **Semantic search** goes beyond keyword matching to understand the intent and meaning behind a query. A user searching for "affordable transportation in urban areas" retrieves results about public transit and ride-sharing, even if those exact words do not appear in the content.
- **Retrieval-augmented generation (RAG)** uses a vector database to retrieve relevant context that is then fed to a large language model to generate grounded, accurate responses. This architecture reduces hallucination and enables LLMs to work with proprietary or up-to-date information.
- **Recommendation systems** encode users and items as vectors and retrieve the nearest items to a user's preference vector, enabling personalized content, product, or media recommendations.
- **Image and multimedia search** allows users to find visually or acoustically similar content by querying with an example image, audio clip, or text description.
- **Anomaly detection** identifies data points whose embeddings are distant from normal clusters, useful in fraud detection, cybersecurity threat identification, and manufacturing quality control.
- **Deduplication and clustering** groups similar records together, supporting use cases such as merging duplicate customer records, organizing document collections, and content moderation.

## Hybrid search and metadata filtering

Pure vector search has limitations. Users often need to combine semantic similarity with structured constraints, such as finding the most relevant product reviews written in the last 30 days, or the most similar medical documents within a specific specialty. Hybrid search addresses this by combining vector similarity with metadata filtering and, in some systems, traditional full-text keyword search.

Most modern vector databases support attaching metadata (key-value pairs, tags, or structured fields) to each vector and applying filters before or after the similarity computation. Pre-filtering restricts the candidate set before vector search, which improves precision but may reduce recall. Post-filtering applies constraints after retrieving the top candidates, preserving recall but potentially returning fewer results than requested.

## Performance and scalability considerations

Deploying a vector database in production requires careful attention to several performance dimensions.

- **Index build time** is the upfront cost of constructing the index. HNSW indexes are fast to query but slower to build than IVF indexes. Incremental indexing, where new vectors can be added without rebuilding the entire index, is important for applications with continuous data ingestion.
- **Query latency** depends on the index type, the number of vectors, the dimensionality, and tuning parameters. Sub-millisecond latency is achievable for datasets in the low millions; larger datasets may require sharding or approximate methods.
- **Memory consumption** is a primary constraint. A dataset of 100 million 768-dimensional float32 vectors requires approximately 300 GB of raw storage, before accounting for index overhead. Quantization techniques such as Product Quantization or scalar quantization can reduce memory usage by 4x to 16x with acceptable recall degradation.
- **Horizontal scaling** distributes data across multiple nodes using sharding. The database routes queries to relevant shards and merges results. Effective sharding strategies are essential for datasets that exceed the capacity of a single machine.
- **Consistency and durability** vary across systems. Some vector databases prioritize availability and eventual consistency, while others offer stronger guarantees. Understanding the consistency model is important for applications where stale or missing results have significant consequences.

## Related

Professionals working with vector databases benefit from studying embedding models and representation learning to understand how vectors are generated. Knowledge of approximate nearest neighbor algorithms provides insight into the trade-offs underlying different index types. Familiarity with retrieval-augmented generation architectures is essential for building LLM-powered applications. Related topics include neural networks, natural language processing, dimensionality reduction algorithms, graph databases, search engine architecture, and distributed database systems.

## Summary

A vector database is purpose-built infrastructure for storing and querying high-dimensional vector embeddings at scale. By employing specialized indexing algorithms such as HNSW, IVF, and product quantization, these systems deliver fast approximate nearest-neighbor search across millions or billions of vectors. They have become foundational to modern AI applications, enabling semantic search, retrieval-augmented generation, recommendation systems, and anomaly detection. As embedding models continue to advance and organizations increasingly rely on AI-driven workflows, vector databases occupy a central and growing role in the technology landscape.

## References

- Johnson, J., Douze, M., & Jegou, H. (2019). "Billion-scale similarity search with GPUs." IEEE Transactions on Big Data. https://arxiv.org/abs/1702.08734
- Malkov, Y. A., & Yashunin, D. A. (2020). "Efficient and robust approximate nearest neighbor search using Hierarchical Navigable Small World graphs." IEEE Transactions on Pattern Analysis and Machine Intelligence. https://arxiv.org/abs/1603.09320
- Pinecone documentation: "What is a Vector Database?" https://www.pinecone.io/learn/vector-database/
- Milvus documentation: https://milvus.io/docs
- Weaviate documentation: https://weaviate.io/developers/weaviate
- Qdrant documentation: https://qdrant.tech/documentation/
- Meta FAISS wiki: https://github.com/facebookresearch/faiss/wiki
- pgvector GitHub repository: https://github.com/pgvector/pgvector
