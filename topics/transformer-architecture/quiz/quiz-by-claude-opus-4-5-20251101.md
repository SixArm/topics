# Transformer architecture

Question: What is the primary purpose of the self-attention mechanism in transformer architecture?

- [ ] To reduce the number of parameters in the neural network
- [ ] To capture long-range dependencies by weighing the importance of different tokens in a sequence
- [ ] To maintain sequential order of tokens without additional encoding
- [ ] To compress input sequences into fixed-length vectors

<details>
  <summary>Answer</summary>
  <p>To capture long-range dependencies by weighing the importance of different tokens in a sequence</p>
  <p>The self-attention mechanism allows the model to weigh how much focus should be given to each word (or token) in a sequence relative to others. This enables transformers to capture long-range dependencies in input sequences, which is a key advantage over architectures that process tokens strictly sequentially. This capability has made transformers highly effective for NLP tasks and has inspired innovations in computer vision and speech recognition.</p>
</details>
