# Generative Pretrained Transformer (GPT)

Question: What is the primary mechanism that enables GPT models to generate human-like text?

- [ ] Supervised learning on labeled datasets from the start
- [ ] Predicting the next word in a sequence based on preceding context
- [ ] Recurrent neural network loops that process words sequentially
- [ ] Rule-based grammar parsing and template matching

<details>
  <summary>Answer</summary>
  <p>Predicting the next word in a sequence based on preceding context</p>
  <p>GPT models are pretrained on massive amounts of text data using unsupervised learning, where they learn to predict the next word in a sequence given the context of preceding words. This next-word prediction capability, combined with the transformer architecture's self-attention mechanisms, enables GPT to generate coherent and contextually appropriate human-like text.</p>
</details>
